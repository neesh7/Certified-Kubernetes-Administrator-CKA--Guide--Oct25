## Multiple Scheduler

we've seen how default scheduler works, it has an algo which distributes pods evenly between nodes by taking consideration about various conditons like, taints and toleration , node affinity and so on.

let's say based on our custom needs we need a seprate scheduling algo , for this requirement we can design our own **k8 scheduler** . While creating a pod or deployment we can instruct k8 to schedule it by using a specific scheduler.

we have 2 options :
- deploy scheduler using binaries
- deploy scheduler as a pod ( Mostly preferred as of today) / deployment


## Deploy Additional scheduler as a pod

my-custom-scheduler.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-custom-scheduler
  namespace: kube-system
spec:
  containers:
  - name: kube-scheduller
    image: k8s.gcr.io/kube-scheduler:v1.29.0
    command:
    - kube-scheduleer
    - --address=127.0.0.1
    - --config=/etc/kubernetes/my-scheduler-config.yaml
```




my-scheduler-config.yml

```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: my-scheduler
clientConnection:
  kubeconfig: "/etc/kubernetes/schduler.conf"
leaderElection:
  leaderElect: true
  resourceNamespace: kube-system
  resourceName: lock-object-my-scheduler
```
leaderelect is needed if mulytiple copy of scheduler is running on different node for HA setup.

üß≠ --kubeconfig=/etc/kubernetes/schduler.conf
‚úÖ Purpose:
Specifies credentials and cluster context for your scheduler to talk to the Kubernetes API server.

üì¶ It contains:
Cluster API endpoint (e.g., https://<api-server-ip>:6443)

TLS certificate & key for authentication

The current context (cluster + user)

üîó Without this:
The scheduler has no way to interact with the cluster (e.g., listing unscheduled pods, watching node states).


‚öôÔ∏è --config=/etc/kubernetes/my-scheduler-config.yaml
‚úÖ Purpose:
Tells the scheduler how to behave ‚Äî e.g., what plugins to use, what the scheduler name is, and whether to do leader election.

üí° Key fields:
clientConnection.kubeconfig: same as --kubeconfig (but inside config file)

schedulerName: must match schedulerName in the pods you want this scheduler to handle

leaderElection: typically false for single-instance custom schedulers

üîÅ Note: If you're using --config, you do not need to separately specify --kubeconfig on the command line ‚Äî it's already included in the config file.

#### Once the scheduler is setup we want to use that while running our pod so that's how we modify our pod manifest file.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: mycontainer
    image: myimage:latest
  schedulerName: my-custom-scheduler
```
```bash 
## commands

# View schedulers
$ kubectl get pods --namespace=kube-system

# checking our pod lauched using our scheduler
$ kubectl get events -o wide

# view logs of scheduler
$ kubectl logs my-custom-scheduler --namespace=kube-systems
```